---
id: models
title: Available Models
sidebar_label: Models
sidebar_position: 4
slug: /models
description: "Explore NEAR AI Cloud model catalog"
---

import DeepseekIcon from '@site/static/img/icons/models/Deepseek-logo-icon.svg';
import GPTIcon from '@site/static/img/icons/models/GPT-logo.svg';
import QwenIcon from '@site/static/img/icons/models/Qwen_logo.svg';
import ZaiIcon from '@site/static/img/icons/models/zai-logo.svg';

NEAR AI Cloud offers a curated catalog of high-performance models spanning reasoning, tool use,
and long-context understanding. Pricing is listed per million tokens for easy comparison.

<div className="doc-model-grid">
  <div className="doc-model-card">
    <div className="doc-model-header">
      <div className="doc-model-icon">
        <DeepseekIcon />
      </div>
      <div>
        <h3>DeepSeek V3.1</h3>
        <p className="doc-model-provider">deepseek-ai/DeepSeek-V3.1</p>
      </div>
    </div>
    <p>
      DeepSeek V3.1 is a hybrid model that supports both thinking mode and non-thinking mode. Compared
      with the previous version, this upgrade delivers gains across multiple areas:
    </p>
    <ul>
      <li><strong>Hybrid thinking mode:</strong> One model supports both thinking and non-thinking modes by changing the chat template.</li>
      <li><strong>Smarter tool calling:</strong> Post-training optimization significantly boosts tool usage and agent task performance.</li>
      <li><strong>Higher thinking efficiency:</strong> DeepSeek-V3.1-Think delivers DeepSeek-R1-0528 quality while responding more quickly.</li>
    </ul>
    <div className="doc-model-meta">
      <span>128K context</span>
      <span>$1.00 /M input tokens</span>
      <span>$2.50 /M output tokens</span>
    </div>
  </div>

  <div className="doc-model-card">
    <div className="doc-model-header">
      <div className="doc-model-icon">
        <GPTIcon />
      </div>
      <div>
        <h3>GPT OSS 120B</h3>
        <p className="doc-model-provider">openai/gpt-oss-120b</p>
      </div>
    </div>
    <p>
      GPT OSS 120B is OpenAI&rsquo;s 117B-parameter Mixture-of-Experts model for production reasoning and
      agentic workflows. It activates just 5.1B parameters per pass, runs efficiently on a single H100
      via native MXFP4 quantization, and supports configurable reasoning depth.
    </p>
    <p>
      You get full chain-of-thought visibility, native tool use (function calling, browsing, structured
      outputs), and high reliability for complex pipelines.
    </p>
    <div className="doc-model-meta">
      <span>131K context</span>
      <span>$0.20 /M input tokens</span>
      <span>$0.60 /M output tokens</span>
    </div>
  </div>

  <div className="doc-model-card">
    <div className="doc-model-header">
      <div className="doc-model-icon">
        <QwenIcon />
      </div>
      <div>
        <h3>Qwen3 30B A3B Instruct 2507</h3>
        <p className="doc-model-provider">Qwen/Qwen3-30B-A3B-Instruct-2507</p>
      </div>
    </div>
    <p>
      Qwen3-30B-A3B-Instruct-2507 is a 30.5B-parameter MoE model (3.3B active per inference) with an
      ultra-long 262K context window. It excels at instruction following, logical reasoning, coding,
      mathematics, multilingual tasks, and preference alignment&mdash;all in non-thinking mode.
    </p>
    <p>
      Use it when you need multilingual comprehension and strong instruction adherence without the
      overhead of a full reasoning model.
    </p>
    <div className="doc-model-meta">
      <span>262K context</span>
      <span>$0.15 /M input tokens</span>
      <span>$0.45 /M output tokens</span>
    </div>
  </div>

  <div className="doc-model-card">
    <div className="doc-model-header">
      <div className="doc-model-icon">
        <ZaiIcon />
      </div>
      <div>
        <h3>GLM-4.6 FP8</h3>
        <p className="doc-model-provider">zai-org/GLM-4.6-FP8</p>
      </div>
    </div>
    <p>
      GLM-4.6 FP8 from Zhipu AI packs 358B parameters into an FP8-quantized deployment with a 128K
      context window. It shines in advanced coding, multi-step reasoning, and tool calling while
      boosting token efficiency by up to 15% versus GLM-4.5.
    </p>
    <p>
      Positioned as a competitor to Claude Sonnet 4 and DeepSeek-V3.1-Terminus, it delivers premium
      writing quality and robust agentic workflow support for production environments.
    </p>
    <div className="doc-model-meta">
      <span>131K context</span>
      <span>$0.75 /M input tokens</span>
      <span>$2.00 /M output tokens</span>
    </div>
  </div>
</div>

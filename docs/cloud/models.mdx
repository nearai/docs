---
id: models
title: Available Models
sidebar_label: Models
slug: /cloud/models
description: "Explore NEAR AI Cloud model catalog"
---

import DeepseekIcon from '@site/static/img/icons/models/Deepseek-logo-icon.svg';
import GPTIcon from '@site/static/img/icons/models/GPT-logo.svg';
import QwenIcon from '@site/static/img/icons/models/Qwen_logo.svg';
import ZaiIcon from '@site/static/img/icons/models/zai-logo.svg';
import CodeBlock from '@theme/CodeBlock';

NEAR AI Cloud provides access to leading AI models, each optimized for different use cases â€” from advanced reasoning and tool calling to long-context processing and multilingual tasks. All models run in secure TEE environments with transparent, pay-per-use pricing.

## Quick Reference

<table>
  <thead>
    <tr>
      <th>Model ID</th>
      <th>Context</th>
      <th>Input Price</th>
      <th>Output Price</th>
      <th>Best For</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><CodeBlock language="text">deepseek-ai/DeepSeek-V3.1</CodeBlock></td>
      <td>128K</td>
      <td>$1.00/M</td>
      <td>$2.50/M</td>
      <td>Hybrid thinking mode, tool calling, agent tasks</td>
    </tr>
    <tr>
      <td><CodeBlock language="text">openai/gpt-oss-120b</CodeBlock></td>
      <td>131K</td>
      <td>$0.20/M</td>
      <td>$0.60/M</td>
      <td>Open-weight, high-reasoning, agentic workflows, configurable depth</td>
    </tr>
    <tr>
      <td><CodeBlock language="text">Qwen/Qwen3-30B-A3B-Instruct-2507</CodeBlock></td>
      <td>262K</td>
      <td>$0.15/M</td>
      <td>$0.45/M</td>
      <td>Ultra-long context (262K), reasoning, instruction following, multilingual</td>
    </tr>
    <tr>
      <td><CodeBlock language="text">zai-org/GLM-4.6-FP8</CodeBlock></td>
      <td>200K</td>
      <td>$0.75/M</td>
      <td>$2.00/M</td>
      <td>Agentic applications, advanced coding, tool use, refined writing</td>
    </tr>
  </tbody>
</table>

---

## Model Details

<div className="doc-model-grid">
  <div className="doc-model-card">
    <div className="doc-model-header">
      <div className="doc-model-icon">
        <DeepseekIcon />
      </div>
      <div>
        <h3>DeepSeek V3.1</h3>
      </div>
    </div>
    <p>
      DeepSeek V3.1 is a hybrid model that supports both thinking mode and non-thinking mode. Compared
      with the previous version, this upgrade delivers gains across multiple areas:
    </p>
    <ul>
      <li><strong>Hybrid thinking mode:</strong> One model supports both thinking and non-thinking modes by changing the chat template.</li>
      <li><strong>Smarter tool calling:</strong> Post-training optimization significantly boosts tool usage and agent task performance.</li>
      <li><strong>Higher thinking efficiency:</strong> DeepSeek-V3.1-Think delivers DeepSeek-R1-0528 quality while responding more quickly.</li>
    </ul>
    <div className="doc-model-meta">
      <span>128K context</span>
      <span>$1.00 /M input tokens</span>
      <span>$2.50 /M output tokens</span>
    </div>
    <p><strong>Model ID:</strong></p>
    <CodeBlock language="text">deepseek-ai/DeepSeek-V3.1</CodeBlock>
  </div>

  <div className="doc-model-card">
    <div className="doc-model-header">
      <div className="doc-model-icon">
        <GPTIcon />
      </div>
      <div>
        <h3>GPT OSS 120B</h3>
      </div>
    </div>
    <p>
      GPT OSS 120B is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI
      designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B
      parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization.
    </p>
    <p>
      The model supports configurable reasoning depth, full chain-of-thought access, and native tool use,
      including function calling, browsing, and structured output generation.
    </p>
    <div className="doc-model-meta">
      <span>131K context</span>
      <span>$0.20 /M input tokens</span>
      <span>$0.60 /M output tokens</span>
    </div>
    <p><strong>Model ID:</strong></p>
    <CodeBlock language="text">openai/gpt-oss-120b</CodeBlock>
  </div>

  <div className="doc-model-card">
    <div className="doc-model-header">
      <div className="doc-model-icon">
        <QwenIcon />
      </div>
      <div>
        <h3>Qwen3 30B A3B Instruct 2507</h3>
      </div>
    </div>
    <p>
      Qwen3-30B-A3B-Instruct-2507 is a mixture-of-experts (MoE) causal language model featuring 30.5 billion
      total parameters and 3.3 billion activated parameters per inference. It supports ultra-long context up
      to 262K tokens and operates exclusively in non-thinking mode, delivering strong enhancements in
      instruction following, reasoning, logical comprehension, mathematics, coding, multilingual understanding,
      and alignment with user preferences.
    </p>
    <div className="doc-model-meta">
      <span>262K context</span>
      <span>$0.15 /M input tokens</span>
      <span>$0.45 /M output tokens</span>
    </div>
    <p><strong>Model ID:</strong></p>
    <CodeBlock language="text">Qwen/Qwen3-30B-A3B-Instruct-2507</CodeBlock>
  </div>

  <div className="doc-model-card">
    <div className="doc-model-header">
      <div className="doc-model-icon">
        <ZaiIcon />
      </div>
      <div>
        <h3>GLM-4.6 FP8</h3>
      </div>
    </div>
    <p>
      GLM-4.6 is the latest flagship model in the GLM (General Language Model) series by Z.ai (formerly Zhipu AI).
      It is oriented toward agentic applications: reasoning, tool usage, coding/engineering workflows, and long-context tasks.
      The FP8 quantized version maintains full performance while optimizing for efficient deployment.
    </p>
    <p>
      Compared with GLM-4.5, GLM-4.6 brings several key improvements: a longer 200K context window (expanded from 128K),
      superior coding performance with better real-world results in applications like Claude Code and Cline, advanced
      reasoning with tool use during inference, more capable search-based agents, and refined writing that better aligns
      with human preferences in style and readability.
    </p>
    <div className="doc-model-meta">
      <span>200K context</span>
      <span>$0.75 /M input tokens</span>
      <span>$2.00 /M output tokens</span>
    </div>
    <p><strong>Model ID:</strong></p>
    <CodeBlock language="text">zai-org/GLM-4.6-FP8</CodeBlock>
  </div>
</div>
